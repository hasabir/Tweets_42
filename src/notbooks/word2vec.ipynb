{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../')))\n",
    "\n",
    "from lib.data_preparation import DataPreparation\n",
    "from lib.preprocessing_data import Preprocessing\n",
    "\n",
    "\n",
    "data_set = DataPreparation().load_data()\n",
    "data_set = Preprocessing().lemmatization(data_set)\n",
    "\n",
    "data_set = data_set.sample(frac=0.02).reset_index(drop=True)\n",
    "processed_tweets = data_set['processed_tweet']\n",
    "\n",
    "# for sentiment, tweet in zip(data_set['sentiment'], data_set['processed_tweet']):\n",
    "#     print(f\"{sentiment}: {tweet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "\n",
    "vocab = set(word for tweet in processed_tweets for word in tweet)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_size = 71\n",
    "learning_rate = 0.01\n",
    "# Initialize embeddings\n",
    "main_embeddings = np.random.normal(0, 0.1, (vocab_size, embedding_size))\n",
    "context_embeddings = np.random.normal(0, 0.1, (vocab_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Numerically stable sigmoid function\n",
    "    return np.where(\n",
    "        x >= 0,\n",
    "        1 / (1 + np.exp(-x)),\n",
    "        np.exp(x) / (1 + np.exp(x))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_samples(vocab_size, exclude_idx, num_samples, word_freq):\n",
    "    \"\"\"Sample negative examples based on word frequencies.\"\"\"\n",
    "    probabilities = np.array([freq ** 0.75 for freq in word_freq])\n",
    "    probabilities /= probabilities.sum()\n",
    "\n",
    "    negative_samples = []\n",
    "    while len(negative_samples) < num_samples:\n",
    "        sampled_idx = np.random.choice(vocab_size, p=probabilities)\n",
    "        if sampled_idx != exclude_idx:\n",
    "            negative_samples.append(sampled_idx) \n",
    "    return negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embeddings(center_idx, context_idx, label):\n",
    "    # Update embeddings for one center-context pair\n",
    "    center_vector = main_embeddings[center_idx]\n",
    "    context_vector = context_embeddings[context_idx]\n",
    "\n",
    "    dot_product = np.dot(center_vector, context_vector)\n",
    "    prediction = sigmoid(dot_product)\n",
    "    error = label - prediction\n",
    "\n",
    "    # Gradient updates\n",
    "    grad_center = error * context_vector\n",
    "    grad_context = error * center_vector\n",
    "\n",
    "    main_embeddings[center_idx] += learning_rate * grad_center\n",
    "    context_embeddings[context_idx] += learning_rate * grad_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def plot_words(word1, word2, word_to_index, main_embeddings, context_embeddings):\n",
    "    \"\"\"\n",
    "    Visualize the embeddings of two words in the main and context embedding spaces.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Subplot 1: Main embeddings\n",
    "    plt.subplot(1, 2, 1)\n",
    "    vector1_main = main_embeddings[word_to_index[word1]]\n",
    "    vector2_main = main_embeddings[word_to_index[word2]]\n",
    "\n",
    "    # Plot vector1\n",
    "    plt.scatter(vector1_main[0], vector1_main[1], color='red', label=word1)\n",
    "    plt.arrow(0, 0, vector1_main[0], vector1_main[1], head_width=0.00, length_includes_head=True, color='red')\n",
    "    plt.text(vector1_main[0] * 1.1, vector1_main[1] * 1.1, word1, fontsize=12)\n",
    "\n",
    "    # Plot vector2\n",
    "    plt.scatter(vector2_main[0], vector2_main[1], color='blue', label=word2)\n",
    "    plt.arrow(0, 0, vector2_main[0], vector2_main[1], head_width=0.00, length_includes_head=True, color='blue')\n",
    "    plt.text(vector2_main[0] * 1.1, vector2_main[1] * 1.1, word2, fontsize=12)\n",
    "\n",
    "    similarity_main = 1 - cosine(vector1_main, vector2_main)\n",
    "    plt.title(f\"Main Embeddings (Similarity = {round(similarity_main, 4)})\", fontsize=4)\n",
    "    plt.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    # Subplot 2: Main vs. Context embeddings\n",
    "    plt.subplot(1, 2, 2)\n",
    "    vector1_context = context_embeddings[word_to_index[word1]]\n",
    "    vector2_context = context_embeddings[word_to_index[word2]]\n",
    "\n",
    "    # Plot vector1\n",
    "    plt.scatter(vector1_main[0], vector1_main[1], color='red', label=f\"{word1} (Main)\")\n",
    "    plt.arrow(0, 0, vector1_main[0], vector1_main[1], head_width=0.00, length_includes_head=True, color='red')\n",
    "    plt.text(vector1_main[0] * 1.1, vector1_main[1] * 1.1, f\"{word1} (Main)\", fontsize=12)\n",
    "\n",
    "    # Plot vector2 from context\n",
    "    plt.scatter(vector2_context[0], vector2_context[1], color='green', label=f\"{word2} (Context)\")\n",
    "    plt.arrow(0, 0, vector2_context[0], vector2_context[1], head_width=0.00, length_includes_head=True, color='green')\n",
    "    plt.text(vector2_context[0] * 1.1, vector2_context[1] * 1.1, f\"{word2} (Context)\", fontsize=12)\n",
    "\n",
    "    similarity_context = 1 - cosine(vector1_main, vector2_context)\n",
    "    plt.title(f\"Main vs. Context (Similarity = {round(similarity_context, 4)})\", fontsize=14)\n",
    "    plt.axvline(0, color='gray')\n",
    "    plt.axhline(0, color='gray')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words2(word1, word2, word_to_index, main_embeddings, context_embeddings):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    import pandas as pd\n",
    "    plt.subplot(1,2,1)\n",
    "    lim_main_first = pd.DataFrame(main_embeddings[word_to_index[word1]])\n",
    "    lim_main_second = pd.DataFrame(main_embeddings[word_to_index[word2]])\n",
    "    p1 = plt.scatter(lim_main_first[0], lim_main_first[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main_first[0]), float(lim_main_first[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main_first.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    p2 = plt.scatter(lim_main_second[0], lim_main_second[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main_second[0]), float(lim_main_second[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main_second.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    sim = 1 - cosine(main_embeddings[word_to_index[word1]], main_embeddings[word_to_index[word2]])\n",
    "    plt.title('Sim = %s'%round(sim,4), fontsize=20)\n",
    "    plt.axvline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    t = np.arange(0, 3.14*2+0.1, 0.1)\n",
    "    plt.plot(np.cos(t), np.sin(t), linewidth=1, color='k', alpha=0.5, linestyle='--')\n",
    "    \n",
    "    ###################################\n",
    "    \n",
    "    # plt.subplot(1,2,2)\n",
    "    # lim_main = main_embeddings.loc[[debug[0]]]\n",
    "    # lim_context = context_embeddings.loc[[debug[1]]]\n",
    "    # p1 = plt.scatter(lim_main[0], lim_main[1], color='r')\n",
    "    # plt.arrow(0,0,float(lim_main[0]), float(lim_main[1]), head_width=0.01, length_includes_head=True)\n",
    "    # for idx,row in lim_main.iterrows():\n",
    "    #     plt.text(row[0], row[1], str(idx))\n",
    "    # p2 = plt.scatter(lim_context[0], lim_context[1], color='b')\n",
    "    # plt.arrow(0,0,float(lim_context[0]), float(lim_context[1]), head_width=0.01, length_includes_head=True)\n",
    "    # for idx,row in lim_context.iterrows():\n",
    "    #     plt.text(row[0], row[1], str(idx))\n",
    "    # sim = 1 - cosine(main_embeddings.loc[debug[0]], context_embeddings.loc[debug[1]])\n",
    "    # plt.title('Sim = %s'%round(sim,4), fontsize=20)\n",
    "    # plt.axvline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    # plt.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # plt.plot(np.cos(t), np.sin(t), linewidth=1, color='k', alpha=0.5, linestyle='--')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words2(word1, word2, word_to_index, main_embeddings, context_embeddings):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.spatial.distance import cosine\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    # Subplot 1: Main embeddings\n",
    "    plt.subplot(1, 2, 1)\n",
    "    vector1_main = main_embeddings[word_to_index[word1]]\n",
    "    vector2_main = main_embeddings[word_to_index[word2]]\n",
    "\n",
    "    # Plot vectors for word1 and word2\n",
    "    plt.scatter(vector1_main[0], vector1_main[1], color='red', label=word1)\n",
    "    plt.arrow(0, 0, vector1_main[0], vector1_main[1], color='red', head_width=0.00, length_includes_head=True)\n",
    "    plt.text(vector1_main[0] * 1.1, vector1_main[1] * 1.1, word1, fontsize=10)\n",
    "\n",
    "    plt.scatter(vector2_main[0], vector2_main[1], color='blue', label=word2)\n",
    "    plt.arrow(0, 0, vector2_main[0], vector2_main[1], color='blue', head_width=0.00, length_includes_head=True)\n",
    "    plt.text(vector2_main[0] * 1.1, vector2_main[1] * 1.1, word2, fontsize=10)\n",
    "\n",
    "    # Add similarity as title\n",
    "    similarity_main = 1 - cosine(vector1_main, vector2_main)\n",
    "    plt.title(f\"Main Embeddings (Sim = {round(similarity_main, 4)})\", fontsize=12)\n",
    "\n",
    "    # Add gridlines and axes\n",
    "    plt.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.grid()\n",
    "\n",
    "    # Set zoomed-in axis limits\n",
    "    plt.xlim(-0.1, 0.1)\n",
    "    plt.ylim(-0.1, 0.1)\n",
    "\n",
    "    # Add optional unit circle reference\n",
    "    t = np.linspace(0, 2 * np.pi, 100)\n",
    "    plt.plot(np.cos(t), np.sin(t), linewidth=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(word for tweet in processed_tweets for word in tweet)\n",
    "word_freq = np.array([word_counts[word] for word in vocab])\n",
    "\n",
    "epochs = 50\n",
    "window_size = 3\n",
    "num_negative_samples=3\n",
    "# Training Loop\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    for tweet in processed_tweets:\n",
    "        for center_idx, center_word in enumerate(tweet):\n",
    "            center_word_idx = word_to_idx[center_word]\n",
    "            start = max(center_idx - window_size, 0)\n",
    "            end = min(center_idx + window_size + 1, len(tweet))\n",
    "\n",
    "            for context_idx in range(start, end):\n",
    "                if center_idx == context_idx:\n",
    "                    continue\n",
    "                context_word_idx = word_to_idx[tweet[context_idx]]\n",
    "\n",
    "                # Positive sample\n",
    "                update_embeddings(center_word_idx, context_word_idx, 1)\n",
    "                # Negative samples\n",
    "                negative_samples = get_negative_samples(vocab_size, center_word_idx, num_negative_samples, word_freq)\n",
    "                for negative_idx in negative_samples:\n",
    "                    update_embeddings(center_word_idx, negative_idx, 0)\n",
    "\n",
    "    # Normalize embeddings after each epoch\n",
    "    main_embeddings = normalize_embeddings(main_embeddings)\n",
    "    context_embeddings = normalize_embeddings(context_embeddings)\n",
    "\n",
    "    # Plot embeddings at the end of every epoch (optional)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} completed\")\n",
    "    if epoch % 5 == 0:\n",
    "        plot_words2('sad', 'unhappy', word_to_idx, main_embeddings, context_embeddings)\n",
    "\n",
    "# Final visualization\n",
    "plot_words2('sad', 'unhappy', word_to_idx, main_embeddings, context_embeddings)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
