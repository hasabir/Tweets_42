{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../')))\n",
    "\n",
    "from lib.data_preparation import DataPreparation\n",
    "from lib.preprocessing_data import Preprocessing\n",
    "\n",
    "\n",
    "data_set = DataPreparation().load_data()\n",
    "data_set = Preprocessing().tokenization(data_set)\n",
    "\n",
    "data_set = data_set.sample(frac=0.02).reset_index(drop=True)\n",
    "processed_tweets = data_set['processed_tweet']\n",
    "print(data_set['tweet'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "\n",
    "vocab = set(word for tweet in processed_tweets for word in tweet)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_size = 71\n",
    "learning_rate = 0.1\n",
    "# Initialize embeddings\n",
    "main_embeddings = np.random.normal(0, 0.1, (vocab_size, embedding_size))\n",
    "context_embeddings = np.random.normal(0, 0.1, (vocab_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Numerically stable sigmoid function\n",
    "    return np.where(\n",
    "        x >= 0,\n",
    "        1 / (1 + np.exp(-x)),\n",
    "        np.exp(x) / (1 + np.exp(x))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_samples(vocab_size, exclude_idx, num_samples, word_freq):\n",
    "    \"\"\"Sample negative examples based on word frequencies.\"\"\"\n",
    "    probabilities = np.array([freq ** 0.75 for freq in word_freq])\n",
    "    probabilities /= probabilities.sum()\n",
    "\n",
    "    negative_samples = []\n",
    "    while len(negative_samples) < num_samples:\n",
    "        sampled_idx = np.random.choice(vocab_size, p=probabilities)\n",
    "        if sampled_idx != exclude_idx:\n",
    "            negative_samples.append(sampled_idx) \n",
    "    return negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embeddings(center_idx, context_idx, label):\n",
    "    # Update embeddings for one center-context pair\n",
    "    center_vector = main_embeddings[center_idx]\n",
    "    context_vector = context_embeddings[context_idx]\n",
    "\n",
    "    dot_product = np.dot(center_vector, context_vector)\n",
    "    prediction = sigmoid(dot_product)\n",
    "    error = label - prediction\n",
    "\n",
    "    # Gradient updates\n",
    "    grad_center = error * context_vector\n",
    "    grad_context = error * center_vector\n",
    "\n",
    "    main_embeddings[center_idx] += learning_rate * grad_center\n",
    "    context_embeddings[context_idx] += learning_rate * grad_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def plot_words(word1, word2, word_to_index, main_embeddings, context_embeddings):\n",
    "    \"\"\"\n",
    "    Visualize the embeddings of two words in the main and context embedding spaces.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Subplot 1: Main embeddings\n",
    "    plt.subplot(1, 2, 1)\n",
    "    vector1_main = main_embeddings[word_to_index[word1]]\n",
    "    vector2_main = main_embeddings[word_to_index[word2]]\n",
    "\n",
    "    # Plot vector1\n",
    "    plt.scatter(vector1_main[0], vector1_main[1], color='red', label=word1)\n",
    "    plt.arrow(0, 0, vector1_main[0], vector1_main[1], length_includes_head=True, color='red')\n",
    "    plt.text(vector1_main[0] * 1.1, vector1_main[1] * 1.1, word1, fontsize=12)\n",
    "\n",
    "    # Plot vector2\n",
    "    plt.scatter(vector2_main[0], vector2_main[1], color='blue', label=word2)\n",
    "    plt.arrow(0, 0, vector2_main[0], vector2_main[1], length_includes_head=True, color='blue')\n",
    "    plt.text(vector2_main[0] * 1.1, vector2_main[1] * 1.1, word2, fontsize=12)\n",
    "\n",
    "    similarity_main = 1 - cosine(vector1_main, vector2_main)\n",
    "    plt.title(f\"Main Embeddings (Similarity = {round(similarity_main, 4)})\", fontsize=14)\n",
    "    plt.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    # Subplot 2: Main vs. Context embeddings\n",
    "    plt.subplot(1, 2, 2)\n",
    "    vector1_context = context_embeddings[word_to_index[word1]]\n",
    "    vector2_context = context_embeddings[word_to_index[word2]]\n",
    "\n",
    "    # Plot vector1\n",
    "    plt.scatter(vector1_main[0], vector1_main[1], color='red', label=f\"{word1} (Main)\")\n",
    "    plt.arrow(0, 0, vector1_main[0], vector1_main[1], length_includes_head=True, color='red')\n",
    "    plt.text(vector1_main[0] * 1.1, vector1_main[1] * 1.1, f\"{word1} (Main)\", fontsize=12)\n",
    "\n",
    "    # Plot vector2 from context\n",
    "    plt.scatter(vector2_context[0], vector2_context[1], color='green', label=f\"{word2} (Context)\")\n",
    "    plt.arrow(0, 0, vector2_context[0], vector2_context[1], length_includes_head=True, color='green')\n",
    "    plt.text(vector2_context[0] * 1.1, vector2_context[1] * 1.1, f\"{word2} (Context)\", fontsize=12)\n",
    "\n",
    "    similarity_context = 1 - cosine(vector1_main, vector2_context)\n",
    "    plt.title(f\"Main vs. Context (Similarity = {round(similarity_context, 4)})\", fontsize=14)\n",
    "    plt.axvline(0, color='gray')\n",
    "    plt.axhline(0, color='gray')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(word for tweet in processed_tweets for word in tweet)\n",
    "word_freq = np.array([word_counts[word] for word in vocab])\n",
    "\n",
    "epochs = 20\n",
    "window_size = 3\n",
    "num_negative_samples=3\n",
    "# Training Loop\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    for tweet in processed_tweets:\n",
    "        for center_idx, center_word in enumerate(tweet):\n",
    "            center_word_idx = word_to_idx[center_word]\n",
    "            start = max(center_idx - window_size, 0)\n",
    "            end = min(center_idx + window_size + 1, len(tweet))\n",
    "\n",
    "            for context_idx in range(start, end):\n",
    "                if center_idx == context_idx:\n",
    "                    continue\n",
    "                context_word_idx = word_to_idx[tweet[context_idx]]\n",
    "\n",
    "                # Positive sample\n",
    "                \n",
    "                update_embeddings(center_word_idx, context_word_idx, 1)\n",
    "                # Negative samples\n",
    "                negative_samples = get_negative_samples(vocab_size, center_word_idx, num_negative_samples, word_freq)\n",
    "                for negative_idx in negative_samples:\n",
    "                    update_embeddings(center_word_idx, negative_idx, 0)\n",
    "\n",
    "    # Normalize embeddings after each epoch\n",
    "    main_embeddings = normalize_embeddings(main_embeddings)\n",
    "    context_embeddings = normalize_embeddings(context_embeddings)\n",
    "\n",
    "    # Plot embeddings at the end of every epoch (optional)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} completed\")\n",
    "    if epochs % 2 == 0:\n",
    "        plot_words('like', 'happy', word_to_idx, main_embeddings, context_embeddings)\n",
    "\n",
    "# Final visualization\n",
    "plot_words('like', 'happy', word_to_idx, main_embeddings, context_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_word_vectors(word_list, embeddings, word_to_idx):\n",
    "    \"\"\"\n",
    "    Visualizes the vectors of given words in 2D space.\n",
    "    \n",
    "    Parameters:\n",
    "        word_list (list): List of words to visualize.\n",
    "        embeddings (numpy.ndarray): Embedding matrix.\n",
    "        word_to_idx (dict): Mapping of words to their indices.\n",
    "    \"\"\"\n",
    "    if len(word_list) > 2:\n",
    "        raise ValueError(\"Please provide exactly two words for visualization.\")\n",
    "    \n",
    "    # Extract vectors for the given words\n",
    "    vectors = np.array([embeddings[word_to_idx[word]] for word in word_list])\n",
    "    \n",
    "    # Plot vectors in 2D space\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    origin = [0, 0]\n",
    "    for word, vector in zip(word_list, vectors):\n",
    "        plt.quiver(\n",
    "            *origin, vector[0], vector[1], scale=1, scale_units='xy', angles='xy',\n",
    "            color=np.random.rand(3,), label=word\n",
    "        )\n",
    "        plt.text(vector[0] * 1.1, vector[1] * 1.1, word, fontsize=12, color='red')\n",
    "    \n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.axhline(0, color='gray', linestyle='--')\n",
    "    plt.axvline(0, color='gray', linestyle='--')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title(\"Word Vector Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "# Example words to visualize (ensure they exist in your vocab)\n",
    "words_to_visualize = ['unhappy', 'better']  # Replace with actual words from your vocabulary\n",
    "plot_word_vectors(words_to_visualize, main_embeddings, word_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Example embeddings (assuming main_embeddings is already populated)\n",
    "# # Replace with actual embedding vectors during training\n",
    "# main_embeddings = np.random.rand(100, 50)  # Example of 100 words with 50-dimensional vectors\n",
    "# context_embeddings = np.random.rand(100, 50)\n",
    "\n",
    "# # Choose two example words to visualize\n",
    "# example_words = [\"thanks\", \"happy\"]  # Replace with actual words in your vocabulary\n",
    "# example_indices = [word_to_idx[word] for word in example_words]\n",
    "\n",
    "# # Get their embeddings\n",
    "# example_vectors = main_embeddings[example_indices]\n",
    "\n",
    "# # Reduce to 2D for visualization using PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# reduced_vectors = pca.fit_transform(example_vectors)\n",
    "\n",
    "# # Visualize the vectors\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for i, word in enumerate(example_words):\n",
    "#     plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1], label=word, s=100)\n",
    "#     plt.text(reduced_vectors[i, 0] + 0.02, reduced_vectors[i, 1], word, fontsize=12)\n",
    "\n",
    "# plt.title(\"Visualization of Word Embeddings\")\n",
    "# plt.xlabel(\"PCA Component 1\")\n",
    "# plt.ylabel(\"PCA Component 2\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
