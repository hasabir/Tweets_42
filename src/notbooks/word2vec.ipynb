{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../')))\n",
    "\n",
    "from lib.data_preparation import DataPreparation\n",
    "from lib.preprocessing_data import Preprocessing\n",
    "\n",
    "\n",
    "data_set = DataPreparation().load_data()\n",
    "data_set = Preprocessing().lemmatization(data_set)\n",
    "\n",
    "data_set = data_set.sample(frac=0.02).reset_index(drop=True)\n",
    "processed_tweets = data_set['processed_tweet']\n",
    "\n",
    "for sentiment, tweet in zip(data_set['sentiment'], data_set['processed_tweet']):\n",
    "    print(f\"{sentiment}: {tweet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "\n",
    "vocab = set(word for tweet in processed_tweets for word in tweet)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_size = 71\n",
    "learning_rate = 0.01\n",
    "# Initialize embeddings\n",
    "main_embeddings = np.random.normal(0, 0.1, (vocab_size, embedding_size))\n",
    "context_embeddings = np.random.normal(0, 0.1, (vocab_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Numerically stable sigmoid function\n",
    "    return np.where(\n",
    "        x >= 0,\n",
    "        1 / (1 + np.exp(-x)),\n",
    "        np.exp(x) / (1 + np.exp(x))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_samples(vocab_size, exclude_idx, num_samples, word_freq):\n",
    "    \"\"\"Sample negative examples based on word frequencies.\"\"\"\n",
    "    probabilities = np.array([freq ** 0.75 for freq in word_freq])\n",
    "    probabilities /= probabilities.sum()\n",
    "\n",
    "    negative_samples = []\n",
    "    while len(negative_samples) < num_samples:\n",
    "        sampled_idx = np.random.choice(vocab_size, p=probabilities)\n",
    "        if sampled_idx != exclude_idx:\n",
    "            negative_samples.append(sampled_idx) \n",
    "    return negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embeddings(center_idx, context_idx, label):\n",
    "    # Update embeddings for one center-context pair\n",
    "    center_vector = main_embeddings[center_idx]\n",
    "    context_vector = context_embeddings[context_idx]\n",
    "\n",
    "    dot_product = np.dot(center_vector, context_vector)\n",
    "    prediction = sigmoid(dot_product)\n",
    "    error = label - prediction\n",
    "\n",
    "    # Gradient updates\n",
    "    grad_center = error * context_vector\n",
    "    grad_context = error * center_vector\n",
    "\n",
    "    main_embeddings[center_idx] += learning_rate * grad_center\n",
    "    context_embeddings[context_idx] += learning_rate * grad_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words2(word1, word2, word_to_index, main_embeddings, context_embeddings):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.spatial.distance import cosine\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    # Subplot 1: Main embeddings\n",
    "    plt.subplot(1, 2, 1)\n",
    "    vector1_main = main_embeddings[word_to_index[word1]]\n",
    "    vector2_main = main_embeddings[word_to_index[word2]]\n",
    "\n",
    "    # Plot vectors for word1 and word2\n",
    "    plt.scatter(vector1_main[0], vector1_main[1], color='red', label=word1)\n",
    "    plt.arrow(0, 0, vector1_main[0], vector1_main[1], color='red', head_width=0.00)\n",
    "\n",
    "    plt.scatter(vector2_main[0], vector2_main[1], color='blue', label=word2)\n",
    "    plt.arrow(0, 0, vector2_main[0], vector2_main[1], color='blue', head_width=0.00)\n",
    "\n",
    "    # Add similarity as title\n",
    "    similarity_main = 1 - cosine(vector1_main, vector2_main)\n",
    "    plt.title(f\"Main Embeddings (Sim = {round(similarity_main, 4)})\", fontsize=12)\n",
    "\n",
    "    # Add gridlines and axes\n",
    "    plt.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.grid()\n",
    "\n",
    "    # Set zoomed-in axis limits\n",
    "    plt.xlim(-0.2, 0.2)\n",
    "    plt.ylim(-0.2, 0.2)\n",
    "\n",
    "    \n",
    "    plt.legend()\n",
    "    # Add optional unit circle reference\n",
    "    t = np.linspace(0, 2 * np.pi, 100)\n",
    "    plt.plot(np.cos(t), np.sin(t), linewidth=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract three most frequent pairs\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "sentiment_pairs = {'positive': [], 'negative': [], 'neutral': []}\n",
    "\n",
    "for sentiment, processed_tweet in zip(data_set['sentiment'], data_set['processed_tweet']):\n",
    "    pairs = list(combinations(processed_tweet, 2))\n",
    "    sentiment_pairs[sentiment].extend(pairs)\n",
    "\n",
    "frequent_pairs = {}\n",
    "for sentiment, pairs in sentiment_pairs.items():\n",
    "    pair_counts = Counter(pairs)\n",
    "    frequent_pairs[sentiment] = pair_counts.most_common(3)  # Top 3 pairs\n",
    "\n",
    "\n",
    "for sentiment, pairs in frequent_pairs.items():\n",
    "    print(f\"{sentiment.upper()} SENTIMENT:\")\n",
    "    for pair, count in pairs:\n",
    "        print(f\"  Pair: {pair}, Count: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(word for tweet in processed_tweets for word in tweet)\n",
    "word_freq = np.array([word_counts[word] for word in vocab])\n",
    "\n",
    "epochs = 50\n",
    "window_size = 3\n",
    "num_negative_samples=3\n",
    "# Training Loop\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    for tweet in processed_tweets:\n",
    "        for center_idx, center_word in enumerate(tweet):\n",
    "            center_word_idx = word_to_idx[center_word]\n",
    "            start = max(center_idx - window_size, 0)\n",
    "            end = min(center_idx + window_size + 1, len(tweet))\n",
    "\n",
    "            for context_idx in range(start, end):\n",
    "                if center_idx == context_idx:\n",
    "                    continue\n",
    "                context_word_idx = word_to_idx[tweet[context_idx]]\n",
    "\n",
    "                # Positive sample\n",
    "                update_embeddings(center_word_idx, context_word_idx, 1)\n",
    "                # Negative samples\n",
    "                negative_samples = get_negative_samples(vocab_size, center_word_idx, num_negative_samples, word_freq)\n",
    "                for negative_idx in negative_samples:\n",
    "                    update_embeddings(center_word_idx, negative_idx, 0)\n",
    "\n",
    "    # Normalize embeddings after each epoch\n",
    "    main_embeddings = normalize_embeddings(main_embeddings)\n",
    "    context_embeddings = normalize_embeddings(context_embeddings)\n",
    "\n",
    "    # Plot embeddings at the end of every epoch (optional)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} completed\")\n",
    "    if epoch % 5 == 0:\n",
    "        plot_words2('thank', 'happy', word_to_idx, main_embeddings, context_embeddings)\n",
    "\n",
    "# Final visualization\n",
    "plot_words2('thank', 'happy', word_to_idx, main_embeddings, context_embeddings)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
