{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                  grooming season dawns madhya pradesh\n",
      "1                                                      \n",
      "2        free market tops real human rights sound great\n",
      "3                                        says bid enter\n",
      "4                                                      \n",
      "                            ...                        \n",
      "66              self dont buy thatalso im furry unhappy\n",
      "67    supreme court terms unfortunate grounds cited ...\n",
      "68                                            huh happy\n",
      "69                          sc refuses stop becoming cm\n",
      "70                            indian wants police cover\n",
      "Name: tweet, Length: 71, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../')))\n",
    "\n",
    "from lib.data_preparation import DataPreparation\n",
    "from lib.preprocessing_data import Preprocessing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_set = DataPreparation().load_data()\n",
    "data_set = Preprocessing().tokenization(data_set)\n",
    "\n",
    "data_set = data_set.sample(frac=0.02).reset_index(drop=True)\n",
    "processed_tweets = data_set['processed_tweet']\n",
    "print(data_set['tweet'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "\n",
    "vocab = set(word for tweet in processed_tweets for word in tweet)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_size = 71\n",
    "learning_rate = 0.1\n",
    "# Initialize embeddings\n",
    "main_embeddings = np.random.normal(0, 0.1, (vocab_size, embedding_size))\n",
    "context_embeddings = np.random.normal(0, 0.1, (vocab_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Numerically stable sigmoid function\n",
    "    return np.where(\n",
    "        x >= 0,\n",
    "        1 / (1 + np.exp(-x)),\n",
    "        np.exp(x) / (1 + np.exp(x))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_samples(vocab_size, exclude_idx, num_samples, word_freq):\n",
    "    \"\"\"Sample negative examples based on word frequencies.\"\"\"\n",
    "    probabilities = np.array([freq ** 0.75 for freq in word_freq])\n",
    "    probabilities /= probabilities.sum()\n",
    "\n",
    "    negative_samples = []\n",
    "    while len(negative_samples) < num_samples:\n",
    "        sampled_idx = np.random.choice(vocab_size, p=probabilities)\n",
    "        if sampled_idx != exclude_idx:\n",
    "            negative_samples.append(sampled_idx) \n",
    "    return negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embeddings(center_idx, context_idx, label):\n",
    "    # Update embeddings for one center-context pair\n",
    "    center_vector = main_embeddings[center_idx]\n",
    "    context_vector = context_embeddings[context_idx]\n",
    "\n",
    "    dot_product = np.dot(center_vector, context_vector)\n",
    "    prediction = sigmoid(dot_product)\n",
    "    error = label - prediction\n",
    "\n",
    "    # Gradient updates\n",
    "    grad_center = error * context_vector\n",
    "    grad_context = error * center_vector\n",
    "\n",
    "    main_embeddings[center_idx] += learning_rate * grad_center\n",
    "    context_embeddings[context_idx] += learning_rate * grad_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def plot_words(debug):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    lim_main_first = main_embeddings.loc[[debug[0]]]\n",
    "    lim_main_second = main_embeddings.loc[[debug[1]]]\n",
    "    p1 = plt.scatter(lim_main_first[0], lim_main_first[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main_first[0]), float(lim_main_first[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main_first.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    p2 = plt.scatter(lim_main_second[0], lim_main_second[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main_second[0]), float(lim_main_second[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main_second.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    sim = 1 - cosine(main_embeddings.loc[debug[0]], main_embeddings.loc[debug[1]])\n",
    "    plt.title('Sim = %s'%round(sim,4), fontsize=20)\n",
    "    plt.axvline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    t = np.arange(0, 3.14*2+0.1, 0.1)\n",
    "    plt.plot(np.cos(t), np.sin(t), linewidth=1, color='k', alpha=0.5, linestyle='--')\n",
    "    \n",
    "    ###################################\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    lim_main = main_embeddings.loc[[debug[0]]]\n",
    "    lim_context = context_embeddings.loc[[debug[1]]]\n",
    "    p1 = plt.scatter(lim_main[0], lim_main[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main[0]), float(lim_main[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    p2 = plt.scatter(lim_context[0], lim_context[1], color='b')\n",
    "    plt.arrow(0,0,float(lim_context[0]), float(lim_context[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_context.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    sim = 1 - cosine(main_embeddings.loc[debug[0]], context_embeddings.loc[debug[1]])\n",
    "    plt.title('Sim = %s'%round(sim,4), fontsize=20)\n",
    "    plt.axvline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.plot(np.cos(t), np.sin(t), linewidth=1, color='k', alpha=0.5, linestyle='--')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m context_word_idx \u001b[38;5;241m=\u001b[39m word_to_idx[tweet[context_idx]]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Positive sample\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mplot_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlike\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhappy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m update_embeddings(center_word_idx, context_word_idx, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Negative samples\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m, in \u001b[0;36mplot_words\u001b[0;34m(debug)\u001b[0m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m lim_main_first \u001b[38;5;241m=\u001b[39m \u001b[43mmain_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m[[debug[\u001b[38;5;241m0\u001b[39m]]]\n\u001b[1;32m     10\u001b[0m lim_main_second \u001b[38;5;241m=\u001b[39m main_embeddings\u001b[38;5;241m.\u001b[39mloc[[debug[\u001b[38;5;241m1\u001b[39m]]]\n\u001b[1;32m     11\u001b[0m p1 \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mscatter(lim_main_first[\u001b[38;5;241m0\u001b[39m], lim_main_first[\u001b[38;5;241m1\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'loc'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(word for tweet in processed_tweets for word in tweet)\n",
    "word_freq = np.array([word_counts[word] for word in vocab])\n",
    "\n",
    "epochs = 10\n",
    "window_size = 3\n",
    "num_negative_samples=3\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    for tweet in processed_tweets:\n",
    "        for center_idx, center_word in enumerate(tweet):\n",
    "            center_word_idx = word_to_idx[center_word]\n",
    "            start = max(center_idx - window_size, 0)\n",
    "            end = min(center_idx + window_size + 1, len(tweet))\n",
    "\n",
    "            for context_idx in range(start, end):\n",
    "                if center_idx == context_idx:\n",
    "                    continue\n",
    "                context_word_idx = word_to_idx[tweet[context_idx]]\n",
    "\n",
    "                # Positive sample\n",
    "                plot_words(('like', 'happy'))\n",
    "                update_embeddings(center_word_idx, context_word_idx, 1)\n",
    "\n",
    "                # Negative samples\n",
    "                negative_samples = get_negative_samples(vocab_size, center_word_idx, num_negative_samples, word_freq)\n",
    "                for negative_idx in negative_samples:\n",
    "                    update_embeddings(center_word_idx, negative_idx, 0)\n",
    "\n",
    "    # Normalize embeddings after each epoch\n",
    "    main_embeddings = normalize_embeddings(main_embeddings)\n",
    "    \n",
    "    context_embeddings = normalize_embeddings(context_embeddings)\n",
    "    # print(f\"Epoch {epoch + 1}/{epochs} completed\")\n",
    "# plot_words(['like', 'happy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_word_vectors(word_list, embeddings, word_to_idx):\n",
    "    \"\"\"\n",
    "    Visualizes the vectors of given words in 2D space.\n",
    "    \n",
    "    Parameters:\n",
    "        word_list (list): List of words to visualize.\n",
    "        embeddings (numpy.ndarray): Embedding matrix.\n",
    "        word_to_idx (dict): Mapping of words to their indices.\n",
    "    \"\"\"\n",
    "    if len(word_list) > 2:\n",
    "        raise ValueError(\"Please provide exactly two words for visualization.\")\n",
    "    \n",
    "    # Extract vectors for the given words\n",
    "    vectors = np.array([embeddings[word_to_idx[word]] for word in word_list])\n",
    "    \n",
    "    # Plot vectors in 2D space\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    origin = [0, 0]\n",
    "    for word, vector in zip(word_list, vectors):\n",
    "        plt.quiver(\n",
    "            *origin, vector[0], vector[1], scale=1, scale_units='xy', angles='xy',\n",
    "            color=np.random.rand(3,), label=word\n",
    "        )\n",
    "        plt.text(vector[0] * 1.1, vector[1] * 1.1, word, fontsize=12, color='red')\n",
    "    \n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.axhline(0, color='gray', linestyle='--')\n",
    "    plt.axvline(0, color='gray', linestyle='--')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title(\"Word Vector Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "# Example words to visualize (ensure they exist in your vocab)\n",
    "words_to_visualize = ['unhappy', 'better']  # Replace with actual words from your vocabulary\n",
    "plot_word_vectors(words_to_visualize, main_embeddings, word_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Example embeddings (assuming main_embeddings is already populated)\n",
    "# # Replace with actual embedding vectors during training\n",
    "# main_embeddings = np.random.rand(100, 50)  # Example of 100 words with 50-dimensional vectors\n",
    "# context_embeddings = np.random.rand(100, 50)\n",
    "\n",
    "# # Choose two example words to visualize\n",
    "# example_words = [\"thanks\", \"happy\"]  # Replace with actual words in your vocabulary\n",
    "# example_indices = [word_to_idx[word] for word in example_words]\n",
    "\n",
    "# # Get their embeddings\n",
    "# example_vectors = main_embeddings[example_indices]\n",
    "\n",
    "# # Reduce to 2D for visualization using PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# reduced_vectors = pca.fit_transform(example_vectors)\n",
    "\n",
    "# # Visualize the vectors\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for i, word in enumerate(example_words):\n",
    "#     plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1], label=word, s=100)\n",
    "#     plt.text(reduced_vectors[i, 0] + 0.02, reduced_vectors[i, 1], word, fontsize=12)\n",
    "\n",
    "# plt.title(\"Visualization of Word Embeddings\")\n",
    "# plt.xlabel(\"PCA Component 1\")\n",
    "# plt.ylabel(\"PCA Component 2\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
