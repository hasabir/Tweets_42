{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             positive  \\\n",
      "0                                 Im really out happy   \n",
      "1   Well fun Thanks stopping giving shit tonight f...   \n",
      "2                           Haha thanks fansnim happy   \n",
      "3     class hall missions heroic dungeons surprised D   \n",
      "4   link Seventeens Yokohama Concert English subs DDL   \n",
      "5                                          Look happy   \n",
      "6                                      Ill like happy   \n",
      "7                          Everyone getting punched D   \n",
      "8               Facts Parrots Minecraft coming soon D   \n",
      "9   think Im probably beyond repair sounds tea eti...   \n",
      "10                                a scan tumblr happy   \n",
      "11                               thanks recent follow   \n",
      "12                      Thanks Lala Much needed happy   \n",
      "13                                                uss   \n",
      "14                RockSoc still going beginning smile   \n",
      "15              inspiration everyone Miss Jones happy   \n",
      "16                           Play part raising funds    \n",
      "17  Hey thanks top new followers week Much appreci...   \n",
      "18                                     Im happy happy   \n",
      "19                                      figures happy   \n",
      "20                lovely lunch menu sure please smile   \n",
      "21  Thanks recent follow Happy connect happy great...   \n",
      "22  Ruby rehomed times months old loves forever ho...   \n",
      "23  Thanks notifying us updated addon plans accord...   \n",
      "\n",
      "                                             negative  \\\n",
      "0                                            feel bad   \n",
      "1                           True Khilado kuch unhappy   \n",
      "2   damn theres people sincerely want attend BH bi...   \n",
      "3                                  miss minmin crying   \n",
      "4                       song always one faves unhappy   \n",
      "5   tooI miss HGVs going past house though nice lo...   \n",
      "6     Okay lang friend promise meant broken  HAHAHAHA   \n",
      "7                            Im building LLVM backend   \n",
      "8                                        delete tweet   \n",
      "9   Q2 Im MH healthcare professional much want hel...   \n",
      "10                                       Know unhappy   \n",
      "11                                    buffalo unhappy   \n",
      "12  froze mid install Waited 3hrs hed pull plug cr...   \n",
      "13                          miss Rockys posts unhappy   \n",
      "14  damn theres people sincerely want attend BH bi...   \n",
      "15                                    bes jup unhappy   \n",
      "16                    post much lately school unhappy   \n",
      "17                               bring MB man unhappy   \n",
      "18                                                      \n",
      "19  Golly gosh simply dreadful havoc must wreaking...   \n",
      "20                         wheres justin miss unhappy   \n",
      "21  girl unhappy thats why Wa ropiwa 1 Sandton unn...   \n",
      "22                              Aww unhappy Im Coeurl   \n",
      "23                                            unhappy   \n",
      "\n",
      "                                              neutral  \n",
      "0   Man arrested truck deaths suspected carried at...  \n",
      "1                                    more Also epaper  \n",
      "2               space debate dissent continues shrink  \n",
      "3        strengthened sense belonging among Calcuttas  \n",
      "4                            became demonetised notes  \n",
      "5                       transferred 1st major shuffle  \n",
      "6           govt decided merge industries departments  \n",
      "7          MMS cried racism Oz probing Indian attacks  \n",
      "8                                         numbers say  \n",
      "9              bypoll put off ruling combines request  \n",
      "10        Make built Westward ho Calcutta estate Read  \n",
      "11                   Rural planned counter azadi call  \n",
      "12                   Sushil Modi lists property chain  \n",
      "13              US NSA HR arrive today hold talks NSA  \n",
      "14                                   more Also epaper  \n",
      "15                      married Maoist Swati Sengupta  \n",
      "16                                        menu judges  \n",
      "17                   44 days extreme Thats prediction  \n",
      "18  General known sharp questions Trumps new top s...  \n",
      "19                  3 shot dead protestors try rescue  \n",
      "20               death 1st solo trip more Also epaper  \n",
      "21           Supreme Court said welfare schemes proof  \n",
      "22  CM Laxmikant Parsekar tenders resignation lost...  \n",
      "23                       Calcutta get mom accept calf  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../')))\n",
    "\n",
    "from lib.data_preparation import DataPreparation\n",
    "\n",
    "\n",
    "data_set = DataPreparation.remove_stopwords(DataPreparation.load_data())\n",
    "data_set = DataPreparation.remove_punctuation(data_set)\n",
    "\n",
    "\n",
    "data_set = data_set.sample(frac=0.02).reset_index(drop=True)\n",
    "print(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "tokenized_data_frame = pd.DataFrame()\n",
    "for column in data_set.columns:\n",
    "    tokenized_data_frame[column] = data_set[column].astype(str).apply(tokenizer.tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             positive  \\\n",
      "0                            [Im, really, out, happy]   \n",
      "1   [Well, fun, Thanks, stopping, giving, shit, to...   \n",
      "2                      [Haha, thanks, fansnim, happy]   \n",
      "3   [class, hall, missions, heroic, dungeons, surp...   \n",
      "4   [link, Seventeens, Yokohama, Concert, English,...   \n",
      "5                                       [Look, happy]   \n",
      "6                                  [Ill, like, happy]   \n",
      "7                     [Everyone, getting, punched, D]   \n",
      "8        [Facts, Parrots, Minecraft, coming, soon, D]   \n",
      "9   [think, Im, probably, beyond, repair, sounds, ...   \n",
      "10                           [a, scan, tumblr, happy]   \n",
      "11                           [thanks, recent, follow]   \n",
      "12                [Thanks, Lala, Much, needed, happy]   \n",
      "13                                              [uss]   \n",
      "14          [RockSoc, still, going, beginning, smile]   \n",
      "15        [inspiration, everyone, Miss, Jones, happy]   \n",
      "16                       [Play, part, raising, funds]   \n",
      "17  [Hey, thanks, top, new, followers, week, Much,...   \n",
      "18                                 [Im, happy, happy]   \n",
      "19                                   [figures, happy]   \n",
      "20         [lovely, lunch, menu, sure, please, smile]   \n",
      "21  [Thanks, recent, follow, Happy, connect, happy...   \n",
      "22  [Ruby, rehomed, times, months, old, loves, for...   \n",
      "23  [Thanks, notifying, us, updated, addon, plans,...   \n",
      "\n",
      "                                             negative  \\\n",
      "0                                         [feel, bad]   \n",
      "1                      [True, Khilado, kuch, unhappy]   \n",
      "2   [damn, theres, people, sincerely, want, attend...   \n",
      "3                              [miss, minmin, crying]   \n",
      "4                 [song, always, one, faves, unhappy]   \n",
      "5   [tooI, miss, HGVs, going, past, house, though,...   \n",
      "6   [Okay, lang, friend, promise, meant, broken, H...   \n",
      "7                       [Im, building, LLVM, backend]   \n",
      "8                                     [delete, tweet]   \n",
      "9   [Q2, Im, MH, healthcare, professional, much, w...   \n",
      "10                                    [Know, unhappy]   \n",
      "11                                 [buffalo, unhappy]   \n",
      "12  [froze, mid, install, Waited, 3hrs, hed, pull,...   \n",
      "13                     [miss, Rockys, posts, unhappy]   \n",
      "14  [damn, theres, people, sincerely, want, attend...   \n",
      "15                                [bes, jup, unhappy]   \n",
      "16              [post, much, lately, school, unhappy]   \n",
      "17                          [bring, MB, man, unhappy]   \n",
      "18                                                 []   \n",
      "19  [Golly, gosh, simply, dreadful, havoc, must, w...   \n",
      "20                    [wheres, justin, miss, unhappy]   \n",
      "21  [girl, unhappy, thats, why, Wa, ropiwa, 1, San...   \n",
      "22                         [Aww, unhappy, Im, Coeurl]   \n",
      "23                                          [unhappy]   \n",
      "\n",
      "                                              neutral  \n",
      "0   [Man, arrested, truck, deaths, suspected, carr...  \n",
      "1                                [more, Also, epaper]  \n",
      "2         [space, debate, dissent, continues, shrink]  \n",
      "3   [strengthened, sense, belonging, among, Calcut...  \n",
      "4                        [became, demonetised, notes]  \n",
      "5                  [transferred, 1st, major, shuffle]  \n",
      "6     [govt, decided, merge, industries, departments]  \n",
      "7   [MMS, cried, racism, Oz, probing, Indian, atta...  \n",
      "8                                      [numbers, say]  \n",
      "9       [bypoll, put, off, ruling, combines, request]  \n",
      "10  [Make, built, Westward, ho, Calcutta, estate, ...  \n",
      "11             [Rural, planned, counter, azadi, call]  \n",
      "12             [Sushil, Modi, lists, property, chain]  \n",
      "13     [US, NSA, HR, arrive, today, hold, talks, NSA]  \n",
      "14                               [more, Also, epaper]  \n",
      "15                 [married, Maoist, Swati, Sengupta]  \n",
      "16                                     [menu, judges]  \n",
      "17             [44, days, extreme, Thats, prediction]  \n",
      "18  [General, known, sharp, questions, Trumps, new...  \n",
      "19           [3, shot, dead, protestors, try, rescue]  \n",
      "20       [death, 1st, solo, trip, more, Also, epaper]  \n",
      "21    [Supreme, Court, said, welfare, schemes, proof]  \n",
      "22  [CM, Laxmikant, Parsekar, tenders, resignation...  \n",
      "23                 [Calcutta, get, mom, accept, calf]  \n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Stemmming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             positive  \\\n",
      "0                            [im, realli, out, happi]   \n",
      "1   [well, fun, thank, stop, give, shit, tonight, ...   \n",
      "2                       [haha, thank, fansnim, happi]   \n",
      "3   [class, hall, mission, heroic, dungeon, surpri...   \n",
      "4   [link, seventeen, yokohama, concert, english, ...   \n",
      "5                                       [look, happi]   \n",
      "6                                  [ill, like, happi]   \n",
      "7                            [everyon, get, punch, d]   \n",
      "8            [fact, parrot, minecraft, come, soon, d]   \n",
      "9   [think, im, probabl, beyond, repair, sound, te...   \n",
      "10                           [a, scan, tumblr, happi]   \n",
      "11                            [thank, recent, follow]   \n",
      "12                   [thank, lala, much, need, happi]   \n",
      "13                                              [uss]   \n",
      "14                 [rocksoc, still, go, begin, smile]   \n",
      "15               [inspir, everyon, miss, jone, happi]   \n",
      "16                           [play, part, rais, fund]   \n",
      "17  [hey, thank, top, new, follow, week, much, app...   \n",
      "18                                 [im, happi, happi]   \n",
      "19                                     [figur, happi]   \n",
      "20            [love, lunch, menu, sure, pleas, smile]   \n",
      "21  [thank, recent, follow, happi, connect, happi,...   \n",
      "22  [rubi, rehom, time, month, old, love, forev, h...   \n",
      "23  [thank, notifi, us, updat, addon, plan, accord...   \n",
      "\n",
      "                                             negative  \\\n",
      "0                                         [feel, bad]   \n",
      "1                      [true, khilado, kuch, unhappi]   \n",
      "2   [damn, there, peopl, sincer, want, attend, bh,...   \n",
      "3                                 [miss, minmin, cri]   \n",
      "4                   [song, alway, one, fave, unhappi]   \n",
      "5   [tooi, miss, hgv, go, past, hous, though, nice...   \n",
      "6   [okay, lang, friend, promis, meant, broken, ha...   \n",
      "7                          [im, build, llvm, backend]   \n",
      "8                                      [delet, tweet]   \n",
      "9   [q2, im, mh, healthcar, profession, much, want...   \n",
      "10                                    [know, unhappi]   \n",
      "11                                 [buffalo, unhappi]   \n",
      "12  [froze, mid, instal, wait, 3hr, hed, pull, plu...   \n",
      "13                       [miss, rocki, post, unhappi]   \n",
      "14  [damn, there, peopl, sincer, want, attend, bh,...   \n",
      "15                                 [be, jup, unhappi]   \n",
      "16                [post, much, late, school, unhappi]   \n",
      "17                          [bring, mb, man, unhappi]   \n",
      "18                                                 []   \n",
      "19  [golli, gosh, simpli, dread, havoc, must, wrea...   \n",
      "20                     [where, justin, miss, unhappi]   \n",
      "21  [girl, unhappi, that, whi, wa, ropiwa, 1, sand...   \n",
      "22                         [aww, unhappi, im, coeurl]   \n",
      "23                                          [unhappi]   \n",
      "\n",
      "                                              neutral  \n",
      "0   [man, arrest, truck, death, suspect, carri, at...  \n",
      "1                                  [more, also, epap]  \n",
      "2            [space, debat, dissent, continu, shrink]  \n",
      "3         [strengthen, sens, belong, among, calcutta]  \n",
      "4                            [becam, demonetis, note]  \n",
      "5                      [transfer, 1st, major, shuffl]  \n",
      "6               [govt, decid, merg, industri, depart]  \n",
      "7        [mm, cri, racism, oz, probe, indian, attack]  \n",
      "8                                       [number, say]  \n",
      "9            [bypol, put, off, rule, combin, request]  \n",
      "10  [make, built, westward, ho, calcutta, estat, r...  \n",
      "11                [rural, plan, counter, azadi, call]  \n",
      "12              [sushil, modi, list, properti, chain]  \n",
      "13       [us, nsa, hr, arriv, today, hold, talk, nsa]  \n",
      "14                                 [more, also, epap]  \n",
      "15                   [marri, maoist, swati, sengupta]  \n",
      "16                                       [menu, judg]  \n",
      "17                   [44, day, extrem, that, predict]  \n",
      "18  [gener, known, sharp, question, trump, new, to...  \n",
      "19             [3, shot, dead, protestor, tri, rescu]  \n",
      "20         [death, 1st, solo, trip, more, also, epap]  \n",
      "21       [suprem, court, said, welfar, scheme, proof]  \n",
      "22  [cm, laxmik, parsekar, tender, resign, lost, s...  \n",
      "23                 [calcutta, get, mom, accept, calf]  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_data = pd.DataFrame()\n",
    "\n",
    "for column in tokenized_data_frame.columns:\n",
    "    stemmed_data[column] = tokenized_data_frame[column].apply(lambda row: [stemmer.stem(word) for word in row])\n",
    "print(stemmed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             positive  \\\n",
      "0                          [I, m, really, out, happy]   \n",
      "1   [well, fun, thank, stop, give, shit, tonight, ...   \n",
      "2                       [haha, thank, fansnim, happy]   \n",
      "3   [class, hall, mission, heroic, dungeon, surpri...   \n",
      "4   [link, Seventeens, Yokohama, Concert, English,...   \n",
      "5                                       [look, happy]   \n",
      "6                                  [ill, like, happy]   \n",
      "7                       [everyone, getting, punch, d]   \n",
      "8          [Facts, Parrots, Minecraft, come, soon, d]   \n",
      "9   [think, I, m, probably, beyond, repair, sound,...   \n",
      "10                           [a, scan, tumblr, happy]   \n",
      "11                            [thank, recent, follow]   \n",
      "12                   [thank, Lala, much, need, happy]   \n",
      "13                                              [uss]   \n",
      "14                 [RockSoc, still, go, begin, smile]   \n",
      "15        [inspiration, everyone, Miss, Jones, happy]   \n",
      "16                          [play, part, raise, fund]   \n",
      "17  [hey, thank, top, new, follower, week, much, a...   \n",
      "18                               [I, m, happy, happy]   \n",
      "19                                    [figure, happy]   \n",
      "20         [lovely, lunch, menu, sure, please, smile]   \n",
      "21  [thank, recent, follow, Happy, connect, happy,...   \n",
      "22  [ruby, rehome, time, month, old, love, forever...   \n",
      "23  [thank, notify, we, update, addon, plan, accor...   \n",
      "\n",
      "                                             negative  \\\n",
      "0                                         [feel, bad]   \n",
      "1                      [true, Khilado, kuch, unhappy]   \n",
      "2   [damn, there, s, people, sincerely, want, atte...   \n",
      "3                                 [miss, minmin, cry]   \n",
      "4                  [song, always, one, fave, unhappy]   \n",
      "5   [tooI, miss, HGVs, go, past, house, though, ni...   \n",
      "6   [okay, lang, friend, promise, mean, break,  , ...   \n",
      "7                        [I, m, build, llvm, backend]   \n",
      "8                                     [delete, tweet]   \n",
      "9   [Q2, I, m, MH, healthcare, professional, much,...   \n",
      "10                                    [know, unhappy]   \n",
      "11                                 [buffalo, unhappy]   \n",
      "12  [freeze, mid, install, wait, 3hrs, he, d, pull...   \n",
      "13                      [miss, Rockys, post, unhappy]   \n",
      "14  [damn, there, s, people, sincerely, want, atte...   \n",
      "15                                [bes, jup, unhappy]   \n",
      "16              [post, much, lately, school, unhappy]   \n",
      "17                          [bring, MB, man, unhappy]   \n",
      "18                                                 []   \n",
      "19  [golly, gosh, simply, dreadful, havoc, must, w...   \n",
      "20                  [where, s, justin, miss, unhappy]   \n",
      "21  [girl, unhappy, that, s, why, Wa, ropiwa, 1, S...   \n",
      "22                       [aww, unhappy, I, m, coeurl]   \n",
      "23                                          [unhappy]   \n",
      "\n",
      "                                              neutral  \n",
      "0   [man, arrest, truck, death, suspect, carry, at...  \n",
      "1                                [more, also, epaper]  \n",
      "2          [space, debate, dissent, continue, shrink]  \n",
      "3       [strengthen, sense, belong, among, Calcuttas]  \n",
      "4                         [become, demonetised, note]  \n",
      "5                     [transfer, 1st, major, shuffle]  \n",
      "6         [govt, decide, merge, industry, department]  \n",
      "7       [MMS, cry, racism, oz, probe, indian, attack]  \n",
      "8                                       [number, say]  \n",
      "9          [bypoll, put, off, rule, combine, request]  \n",
      "10  [make, build, Westward, ho, Calcutta, estate, ...  \n",
      "11                [rural, plan, counter, azadi, call]  \n",
      "12              [Sushil, Modi, list, property, chain]  \n",
      "13      [US, NSA, HR, arrive, today, hold, talk, NSA]  \n",
      "14                               [more, also, epaper]  \n",
      "15                   [marry, Maoist, Swati, Sengupta]  \n",
      "16                                      [menu, judge]  \n",
      "17            [44, day, extreme, that, s, prediction]  \n",
      "18  [general, know, sharp, question, trump, new, t...  \n",
      "19           [3, shoot, dead, protestor, try, rescue]  \n",
      "20       [death, 1st, solo, trip, more, also, epaper]  \n",
      "21      [Supreme, Court, say, welfare, scheme, proof]  \n",
      "22  [CM, Laxmikant, Parsekar, tender, resignation,...  \n",
      "23                 [Calcutta, get, mom, accept, calf]  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "lemmatized_words = pd.DataFrame()\n",
    "for column in data_set.columns:\n",
    "    lemmatized_words[column] = data_set[column].astype(str).apply(\n",
    "        lambda row: [token.lemma_ for token in nlp(row)]\n",
    "    )\n",
    "    \n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### Stemming + misspellings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### misspelling correction with jaccard_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import words\n",
    "import nltk\n",
    "\n",
    "nltk.download('words')\n",
    "\n",
    "correct_words = set(w for w in words.words() if len(w) > 1)  # Only use words with at least 2 characters\n",
    "stemmer = PorterStemmer()\n",
    "corrected_stemmed_data = pd.DataFrame()\n",
    "\n",
    "# for column in tokenized_data_frame.columns:\n",
    "#     corrected_stemmed_data[column] = tokenized_data_frame[column].apply(\n",
    "#         lambda row: [\n",
    "#             stemmer.stem(min(\n",
    "#                 ((jaccard_distance(set(ngrams(word, 2)), set(ngrams(w, 2))), w) \n",
    "#                  for w in correct_words if len(word) > 1),  # Ensure 'word' has at least 2 chars\n",
    "#                 key=lambda x: x[0]\n",
    "#             )[1]) for word in row if len(word) > 1  # Skip short words entirely\n",
    "#         ] if isinstance(row, list) else row\n",
    "    # )\n",
    "from nltk.corpus import words \n",
    "\n",
    "correct_words = words.words()\n",
    "\n",
    "for column in tokenized_data_frame.columns:\n",
    "    for row in tokenized_data_frame[column]:\n",
    "        print(row)\n",
    "        bigrams = set(ngrams(row, 2))\n",
    "        print(set(ngrams(row, 2)))\n",
    "        for word in correct_words:\n",
    "            print(jaccard_distance(bigrams, set(ngrams(word, 2))))\n",
    "            print()\n",
    "        # if word_bigrams:\n",
    "        # distances = [\n",
    "        #     (jaccard_distance(bigrams, set(ngrams(w, 2))), w)\n",
    "        #     for w in correct_words\n",
    "        #     if set(ngrams(w, 2))\n",
    "        # ]\n",
    "        # if distances:\n",
    "        #     print(min(distances, key=lambda x: x[0])[1])\n",
    "        # print(distances)\n",
    "        # print(min(distances, key=lambda x: x[0])[1] if distances else row)\n",
    "        break\n",
    "    break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.metrics.distance  import edit_distance \n",
    "\n",
    "correct_words = words.words()\n",
    "stemmer = PorterStemmer()\n",
    "corrected_stemmed_words = pd.DataFrame()\n",
    "for columms in tokenized_data_frame.columns:\n",
    "    corrected_data = []\n",
    "    for word in data:\n",
    "        temp = [(edit_distance(word, w),w) for w in correct_words if w[0]==word[0]] \n",
    "        closest_word = min(temp, key=lambda x: x[0])[1] if temp else word\n",
    "        stemmed_word = stemmer.stem(closest_word)\n",
    "        corrected_data.append(stemmed_word)\n",
    "    stemmed_words.append(corrected_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
