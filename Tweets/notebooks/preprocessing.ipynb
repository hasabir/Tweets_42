{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../')))\n",
    "from  src.data_cleaning import DataCleaning\n",
    "\n",
    "data_frame = DataCleaning.load_data()\n",
    "# data_frame_cleaned_from_stop_words = DataCleaning.remove_stopwords(data_frame)\n",
    "\n",
    "\n",
    "\n",
    "# data_frame = data_frame.sample(frac=0.02)\n",
    "\n",
    "# data_set = DataCleaning.remove_stopwords(data_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(data_frame) -> list:\n",
    "    tokenizer = tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens = []\n",
    "    for data in data_frame:\n",
    "        tokens.append(tokenizer.tokenize(data))\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1186, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tokens_positive = tokenization(data_frame=data_frame[\"positive\"])\n",
    "pd.DataFrame(tokens_positive).shape\n",
    "\n",
    "\n",
    "# print(tokens_positive[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data_frame) -> list:\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = []\n",
    "    tokenized_data_frame = tokenization(data_frame)\n",
    "    for data in tokenized_data_frame:\n",
    "        stemmed_words.append([stemmer.stem(word) for word in data])\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = stemming(data_frame=data_frame[\"positive\"])\n",
    "# print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "def lemmatization(data_frame) -> list:\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    lemmatized_words = []\n",
    "    for data in data_frame:\n",
    "        data = nlp(data)\n",
    "        lemmatized_words.append([token.lemma_ for token in data])\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmaized_words = lemmatization(data_frame=data_frame[\"positive\"])\n",
    "print(lemmaized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming + misspellings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### misspelling correction with jaccard_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import nltk \n",
    "from nltk.metrics.distance import jaccard_distance \n",
    "from nltk.util import ngrams\n",
    "nltk.download('words') \n",
    "from nltk.corpus import words \n",
    "\n",
    "def stemming_with_misspelling_correction(data_frame) -> list:\n",
    "    correct_words = words.words()\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = []\n",
    "    tokenized_data_frame = tokenization(data_frame)\n",
    "    for data in tokenized_data_frame:\n",
    "        corrected_data = []\n",
    "        for word in data:\n",
    "            distances = []\n",
    "            word_bigrams = set(ngrams(word, 2))\n",
    "            if word_bigrams:\n",
    "                distances = [\n",
    "                    (jaccard_distance(word_bigrams, set(ngrams(w, 2))), w)\n",
    "                    for w in correct_words\n",
    "                    if set(ngrams(w, 2))\n",
    "                ]\n",
    "            closest_word = min(distances, key=lambda x: x[0])[1] if distances else word\n",
    "            stemmed_word = stemmer.stem(closest_word)\n",
    "            corrected_data.append(stemmed_word)\n",
    "        stemmed_words.append(corrected_data)\n",
    "    return stemmed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stemming(data_frame=data_frame[\"positive\"][:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit distance Method (Levenshtein distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.metrics.distance  import edit_distance \n",
    "\n",
    "def stemming_with_levenshtein_distance(data_frame) -> list:\n",
    "    correct_words = words.words()\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = []\n",
    "    tokenized_data_frame = tokenization(data_frame)\n",
    "    for data in tokenized_data_frame:\n",
    "        corrected_data = []\n",
    "        for word in data:\n",
    "            temp = [(edit_distance(word, w),w) for w in correct_words if w[0]==word[0]] \n",
    "            closest_word = min(temp, key=lambda x: x[0])[1] if temp else word\n",
    "            stemmed_word = stemmer.stem(closest_word)\n",
    "            corrected_data.append(stemmed_word)\n",
    "        stemmed_words.append(corrected_data)\n",
    "    return stemmed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stemming_with_levenshtein_distance(data_frame=data_frame[\"positive\"][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with misspelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics.distance import edit_distance\n",
    "import nltk\n",
    "\n",
    "nltk.download('words')\n",
    "\n",
    "def lemmatization_with_misspelling(data_frame):\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    lemmatized_words = []\n",
    "    correct_words = words.words()\n",
    "    \n",
    "    for data in data_frame:\n",
    "        data = nlp(data)\n",
    "        corrected_data = []\n",
    "        \n",
    "        for word in data:\n",
    "            word_text = word.text\n",
    "            temp = [(edit_distance(word_text, w), w) for w in correct_words if w[0] == word_text[0]]\n",
    "            closest_word = min(temp, key=lambda x: x[0])[1] if temp else word_text\n",
    "            lemma = nlp(closest_word)[0].lemma_\n",
    "            corrected_data.append(lemma)\n",
    "        \n",
    "        lemmatized_words.append(corrected_data)\n",
    "    \n",
    "    return lemmatized_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatization_with_misspelling(data_frame=data_frame[\"positive\"][:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Slang words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ekphrasis_preprocessing(data_set) -> list:\n",
    "    from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "    from ekphrasis.dicts.emoticons import emoticons\n",
    "    from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "\n",
    "    text_processor = TextPreProcessor(\n",
    "        normalize=['url', 'email', 'percent', 'money', 'phone', 'time', 'date', 'number'],\n",
    "        annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\", \"emphasis\", \"censored\"},\n",
    "        fix_html=True,\n",
    "        unpack_hashtags=True,\n",
    "        unpack_contractions=True,\n",
    "        spell_correct_elong=True,\n",
    "        tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "        dicts=[emoticons]\n",
    "    )\n",
    "    processed_data = []\n",
    "    for data in data_set:\n",
    "        processed_data.append(text_processor.pre_process_doc(data))\n",
    "    return processed_data\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['how', 'unhappy', 'some', 'dogs', 'like', 'it', 'though'],\n",
       " ['talking',\n",
       "  'to',\n",
       "  'my',\n",
       "  'over',\n",
       "  'driver',\n",
       "  'about',\n",
       "  'where',\n",
       "  'i',\n",
       "  'am',\n",
       "  'goinghe',\n",
       "  'said',\n",
       "  'he',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'love',\n",
       "  'to',\n",
       "  'go',\n",
       "  'to',\n",
       "  'new',\n",
       "  'york',\n",
       "  'too',\n",
       "  'but',\n",
       "  'since',\n",
       "  'trump',\n",
       "  'it',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'probably',\n",
       "  'not'],\n",
       " ['does',\n",
       "  'anybody',\n",
       "  'know',\n",
       "  'if',\n",
       "  'the',\n",
       "  'rand',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'likely',\n",
       "  'to',\n",
       "  'fall',\n",
       "  'against',\n",
       "  'the',\n",
       "  'dollar',\n",
       "  '?',\n",
       "  'i',\n",
       "  'got',\n",
       "  'some',\n",
       "  'money',\n",
       "  'i',\n",
       "  'need',\n",
       "  'to',\n",
       "  'change',\n",
       "  'into',\n",
       "  'r',\n",
       "  'but',\n",
       "  'it',\n",
       "  'keeps',\n",
       "  'getting',\n",
       "  'stronger',\n",
       "  'unhappy'],\n",
       " ['i', 'miss', 'going', 'to', 'gigs', 'in', 'liverpool', 'unhappy'],\n",
       " ['there', 'isnt', 'a', 'new', 'riverdale', 'tonight', '?', 'unhappy']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ekphrasis_preprocessing(data_frame[\"negative\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['how', 'unhappy', 'some', 'dogs', 'like', 'it', 'though'],\n",
       " ['talking',\n",
       "  'to',\n",
       "  'my',\n",
       "  'over',\n",
       "  'driver',\n",
       "  'about',\n",
       "  'where',\n",
       "  'i',\n",
       "  'am',\n",
       "  'goinghe',\n",
       "  'said',\n",
       "  'he',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'love',\n",
       "  'to',\n",
       "  'go',\n",
       "  'to',\n",
       "  'new',\n",
       "  'york',\n",
       "  'too',\n",
       "  'but',\n",
       "  'since',\n",
       "  'trump',\n",
       "  'it',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'probably',\n",
       "  'not'],\n",
       " ['does',\n",
       "  'anybody',\n",
       "  'know',\n",
       "  'if',\n",
       "  'the',\n",
       "  'rand',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'likely',\n",
       "  'to',\n",
       "  'fall',\n",
       "  'against',\n",
       "  'the',\n",
       "  'dollar',\n",
       "  '?',\n",
       "  'i',\n",
       "  'got',\n",
       "  'some',\n",
       "  'money',\n",
       "  'i',\n",
       "  'need',\n",
       "  'to',\n",
       "  'change',\n",
       "  'into',\n",
       "  'r',\n",
       "  'but',\n",
       "  'it',\n",
       "  'keeps',\n",
       "  'getting',\n",
       "  'stronger',\n",
       "  'unhappy'],\n",
       " ['i', 'miss', 'going', 'to', 'gigs', 'in', 'liverpool', 'unhappy'],\n",
       " ['there', 'isnt', 'a', 'new', 'riverdale', 'tonight', '?', 'unhappy']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ekphrasis_preprocessing(data_frame_cleaned_from_stop_words[\"negative\"][:5])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
